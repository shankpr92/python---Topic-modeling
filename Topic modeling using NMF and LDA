import pandas as pd
df = pd.read_excel("G:\\study\\DS\\mini project\\blockchain_data.xlsx")
/*Extract columns needed for topic modeling*/
#Extracting the AB (Abstract) column
ab = df['AB']
abstracts = ''.join(map(str,ab))
#Extracting the DE (Keywords) column
de = df['DE']
keywords = ''.join(map(str,de))
#Extracting the TI (Title) column
ti = df['TI']
title = ''.join(map(str,ti))
#Combining all the three columns
words = abstracts + keywords + title
#Removing punctuations and digits, converting to lowercase, removing stopwords, lemmatize
import string
p = string.punctuation
d = string.digits
combined = p + d
table = str.maketrans(combined, len(combined) * " ")
stopwords = nltk.corpus.stopwords.words("english")
import nltk
from nltk.stem import WordNetLemmatizer
lemas = WordNetLemmatizer()
word_lst = []
for word in words:
    word_edit = word.translate(table)
    word_lower = word_edit.lower()
    word_list = word_lower.split()
    for word_count in word_list:
        if word_count not in stopwords:
            word_count = lemas.lemmatize(word_count)
            word_lst.append(str(word_count))
#Generate WordCloud
import matplotlib.pyplot as plt
% matplotlib inline
from wordcloud import WordCloud
words_final = ' '.join(map(str,words_lst))
wordcloud = WordCloud().generate(words_final)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
#Getting word counts using Tfidf Vectorizer
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 2)
dtm = vectorizer.fit_transform(words_lemm)
words_vocab = vectorizer.get_feature_names()
print(words_vocab)
print(dtm.toarray())
#Topic modelling using NMF
from sklearn import decomposition
num_topics = 25
num_top_words = 20
words_vocab = vectorizer.get_feature_names()
clf = decomposition.NMF(n_components = num_topics, random_state=42)
doctopic = clf.fit_transform(dtm)
topic_words = []
for topic in clf.components_:
    word_idx = np.argsort(topic)[::-1][0:num_top_words]
    topic_words.append([words_vocab[i] for i in word_idx])
for t in range(len(topic_words)):
    print("Topic {}: {}".format(t, ' '.join(topic_words[t][:15])))
#Topic modelling using LDA
from sklearn.decomposition import LatentDirichletAllocation
vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=10000, stop_words='english')
num_topics = 25
dtm = vectorizer.fit_transform(words_lemm)
lda = LatentDirichletAllocation(n_components=num_topics, learning_method="batch", max_iter=10, random_state=20)
document_topics = lda.fit_transform(dtm)
feature_names = vectorizer.get_feature_names()
def display_topics(model, feature_names, n_top_words):
    for idx, topic in enumerate(model.components_):
        print("Topic %d:" % (idx))
        print(" ".join([feature_names[i]
            for i in topic.argsort()[:-n_top_words - 1:-1]]))
display_topics(lda, feature_names, 20)
#Plot 100 frequent words
plt.figure(figsize=(18,10))
freq = nltk.FreqDist(words_lemm)
freq.plot(100)
# Log Likelihood: Higher the better
print("Log Likelihood: ", lda.score(dtm))
# Perplexity: Lower the better.
print("Perplexity: ", lda.perplexity(dtm))
# See model parameters
print(lda.get_params())
#Use GridSearch to find the best LDA model
from sklearn.model_selection import GridSearchCV
# Define Search Param
search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}
# Init the Model
lda_model = LatentDirichletAllocation()
# Init Grid Search Class
model = GridSearchCV(lda_model, param_grid=search_params)
# Do the Grid Search
model.fit(dtm)
#Best Model
best_lda_model = model.best_estimator_
# Model Parameters
print("Best Model's Params: ", model.best_params_)
# Log Likelihood Score
print("Best Log Likelihood Score: ", model.best_score_)
# Perplexity
print("Model Perplexity: ", best_lda_model.perplexity(dtm))
#Runing the Best LDA model as suggested by GridSearch
lda1 = LatentDirichletAllocation(n_components=10, learning_method="batch", max_iter=10, random_state=20, learning_decay=0.7)
document_topics1 = lda1.fit_transform(dtm)
display_topics(lda1, feature_names, 20)
